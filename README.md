# TML Assignment 3
## Robustness against FGSM and PGD attacks

This assignment trains a robust classifier using the access to a training dataset available.

## Data accessible to the adversary
- Partial training data of the victim model.
- Information on the structure of the dataset.
- Attacks done by the adversary on the trained model (FGSM and PGD).

## Approach used
The implemented approach can be accessed in the ____fill this____ file. The main goal of the assignment is to train a classifier that can be robust against the adversarial examples generated by the adversary using FGSM and PGD attacks while keeping into consideration the accuracy-robustness tradeoff. To obtain a better tradeoff, we use TRADES loss function, which minimizes the regularized surrogate loss, like cross entropy loss in our case, for doing the adversarial training.The trade-off regularization parameter beta is introduced in this loss function to control the robustness of the model. Since, we intend to train the most robust model, we use a beta value 0f 6.0 out of the range of 0.0 to 6.0, where 6.0 suggests high robustness.

## Results
The above approach results in a clean accuracy of 59.8%, robustness (FGSM) - 29.8% and robustness (PGD) - 7%.

## Other implemented ideas
We used FGSM and PGD for adversarial training, which resulted in a clean accuracy of less than 50% and very less robust models.
