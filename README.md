# TML Assignment 3
## Robustness against FGSM and PGD attacks

This assignment trains a robust classifier using access to a training dataset available.

## Data accessible to the adversary
- Partial training data of the victim model.
- Information on the structure of the dataset.
- Attacks done by the adversary on the trained model (FGSM and PGD).

## Approach used
The implemented approach can be accessed in the ____fill this____ file. The main goal of the assignment is to train a classifier that can be robust against the adversarial examples generated by the adversary using FGSM and PGD attacks while keeping into consideration the accuracy-robustness tradeoff. To obtain a better tradeoff, we use the TRADES loss function, which minimizes the regularized surrogate loss, like cross entropy loss in our case, for doing the adversarial training. The trade-off regularization parameter beta is introduced in this loss function to control the robustness of the model. Since we intend to train the most robust model, we use a beta value 0f 6.0 out of the range of 0.0 to 6.0, where 6.0 suggests high robustness. We train the pre-trained resnet50 model for about 100 epochs. We chose resnet50 out of all the allowed models (resnet18, resnet34 and resnet50) since deeper models tend to be more robust. We choose the other hyperparameters by finetuning and referring to the values used in the official implementation of the TRADES loss function.

## Results
The above approach results in a clean accuracy of 62.1%, robustness (FGSM) - 32.8% and robustness (PGD) - 1.4666666666666666%.

## Other implemented ideas
We used FGSM and PGD for adversarial training, which resulted in a clean accuracy of less than 50% and very less robust models.
